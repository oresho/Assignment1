{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oresho/Assignment1/blob/main/speech2Rasp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Gg48JtJXEtiu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be3d137c-1b7f-47f3-b1ac-ebf0574be8e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting python_speech_features\n",
            "  Downloading python_speech_features-0.6.tar.gz (5.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: python_speech_features\n",
            "  Building wheel for python_speech_features (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python_speech_features: filename=python_speech_features-0.6-py3-none-any.whl size=5889 sha256=36d0a7cb5729d6a000f6a9e160f2d9f4694755d5a97385fb858b1c3788ea8ce5\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/60/87/28af2605138deac93d162904df42b6fdda1dab9b8757c62aa3\n",
            "Successfully built python_speech_features\n",
            "Installing collected packages: python_speech_features\n",
            "Successfully installed python_speech_features-0.6\n"
          ]
        }
      ],
      "source": [
        "!pip install python_speech_features\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import isdir, join\n",
        "import librosa\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import python_speech_features\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.utils.get_file('speech_commands_v0.02.tar.gz',\n",
        "                        'http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz',\n",
        "                        cache_dir='./',\n",
        "                        cache_subdir='dataset-speech',\n",
        "                        extract=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "3SpHqenkIlnT",
        "outputId": "6ca06c20-4eb0-4eb7-9df0-cd2966b7ce52"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz\n",
            "2428923189/2428923189 [==============================] - 31s 0us/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./dataset-speech/speech_commands_v0.02.tar.gz'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset path and view possible targets\n",
        "dataset_path = './dataset-speech'\n",
        "for name in listdir(dataset_path):\n",
        "    if isdir(join(dataset_path, name)):\n",
        "        print(name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeBPcmQMJatO",
        "outputId": "338da9d9-3071-4dfe-f4b8-728a0ab39450"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sheila\n",
            "five\n",
            "dog\n",
            "_background_noise_\n",
            "stop\n",
            "one\n",
            "left\n",
            "off\n",
            "eight\n",
            "forward\n",
            "wow\n",
            "seven\n",
            "marvin\n",
            "on\n",
            "four\n",
            "learn\n",
            "six\n",
            "nine\n",
            "no\n",
            "yes\n",
            "cat\n",
            "zero\n",
            "follow\n",
            "three\n",
            "down\n",
            "bed\n",
            "up\n",
            "visual\n",
            "house\n",
            "go\n",
            "tree\n",
            "right\n",
            "two\n",
            "happy\n",
            "bird\n",
            "backward\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an all targets list\n",
        "all_targets = [name for name in listdir(dataset_path) if isdir(join(dataset_path, name))]\n",
        "print(all_targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McRhRghRKP1Y",
        "outputId": "dbfceeec-fb99-4889-ad94-dc073f011005"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['sheila', 'five', 'dog', '_background_noise_', 'stop', 'one', 'left', 'off', 'eight', 'forward', 'wow', 'seven', 'marvin', 'on', 'four', 'learn', 'six', 'nine', 'no', 'yes', 'cat', 'zero', 'follow', 'three', 'down', 'bed', 'up', 'visual', 'house', 'go', 'tree', 'right', 'two', 'happy', 'bird', 'backward']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Leave off background noise set\n",
        "all_targets.remove('_background_noise_')\n",
        "print(all_targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PW3mrxWkKVpJ",
        "outputId": "d4685f82-dd49-438e-bf9a-a45660f54f4c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['sheila', 'five', 'dog', 'stop', 'one', 'left', 'off', 'eight', 'forward', 'wow', 'seven', 'marvin', 'on', 'four', 'learn', 'six', 'nine', 'no', 'yes', 'cat', 'zero', 'follow', 'three', 'down', 'bed', 'up', 'visual', 'house', 'go', 'tree', 'right', 'two', 'happy', 'bird', 'backward']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# See how many files are in each\n",
        "num_samples = 0\n",
        "for target in all_targets:\n",
        "    print(len(listdir(join(dataset_path, target))))\n",
        "    num_samples += len(listdir(join(dataset_path, target)))\n",
        "print('Total samples:', num_samples)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvwhPkp4KX5o",
        "outputId": "d914e11e-92e2-4610-bb82-c046198d8db9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022\n",
            "4052\n",
            "2128\n",
            "3872\n",
            "3890\n",
            "3801\n",
            "3745\n",
            "3787\n",
            "1557\n",
            "2123\n",
            "3998\n",
            "2100\n",
            "3845\n",
            "3728\n",
            "1575\n",
            "3860\n",
            "3934\n",
            "3941\n",
            "4044\n",
            "2031\n",
            "4052\n",
            "1579\n",
            "3727\n",
            "3917\n",
            "2014\n",
            "3723\n",
            "1592\n",
            "2113\n",
            "3880\n",
            "1759\n",
            "3778\n",
            "3880\n",
            "2054\n",
            "2064\n",
            "1664\n",
            "Total samples: 105829\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Settings\n",
        "target_list = all_targets\n",
        "feature_sets_file = 'all_targets_mfcc_sets.npz'\n",
        "perc_keep_samples = 1.0 # 1.0 is keep all samples\n",
        "val_ratio = 0.1\n",
        "test_ratio = 0.1\n",
        "sample_rate = 8000\n",
        "num_mfcc = 16\n",
        "len_mfcc = 16"
      ],
      "metadata": {
        "id": "2-a5XSszLRb1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create list of filenames along with ground truth vector (y)\n",
        "filenames = []\n",
        "y = []\n",
        "for index, target in enumerate(target_list):\n",
        "    print(join(dataset_path, target))\n",
        "    filenames.append(listdir(join(dataset_path, target)))\n",
        "    y.append(np.ones(len(filenames[index])) * index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGw5JuySLVO-",
        "outputId": "336af69f-9f7c-4ef9-f28f-8298acc1e8bd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./dataset-speech/sheila\n",
            "./dataset-speech/five\n",
            "./dataset-speech/dog\n",
            "./dataset-speech/stop\n",
            "./dataset-speech/one\n",
            "./dataset-speech/left\n",
            "./dataset-speech/off\n",
            "./dataset-speech/eight\n",
            "./dataset-speech/forward\n",
            "./dataset-speech/wow\n",
            "./dataset-speech/seven\n",
            "./dataset-speech/marvin\n",
            "./dataset-speech/on\n",
            "./dataset-speech/four\n",
            "./dataset-speech/learn\n",
            "./dataset-speech/six\n",
            "./dataset-speech/nine\n",
            "./dataset-speech/no\n",
            "./dataset-speech/yes\n",
            "./dataset-speech/cat\n",
            "./dataset-speech/zero\n",
            "./dataset-speech/follow\n",
            "./dataset-speech/three\n",
            "./dataset-speech/down\n",
            "./dataset-speech/bed\n",
            "./dataset-speech/up\n",
            "./dataset-speech/visual\n",
            "./dataset-speech/house\n",
            "./dataset-speech/go\n",
            "./dataset-speech/tree\n",
            "./dataset-speech/right\n",
            "./dataset-speech/two\n",
            "./dataset-speech/happy\n",
            "./dataset-speech/bird\n",
            "./dataset-speech/backward\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check ground truth Y vector\n",
        "print(y)\n",
        "for item in y:\n",
        "    print(len(item))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKoVlRolLoZX",
        "outputId": "20e6b9ff-331d-4e5b-ae4b-f815ebba17df"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([0., 0., 0., ..., 0., 0., 0.]), array([1., 1., 1., ..., 1., 1., 1.]), array([2., 2., 2., ..., 2., 2., 2.]), array([3., 3., 3., ..., 3., 3., 3.]), array([4., 4., 4., ..., 4., 4., 4.]), array([5., 5., 5., ..., 5., 5., 5.]), array([6., 6., 6., ..., 6., 6., 6.]), array([7., 7., 7., ..., 7., 7., 7.]), array([8., 8., 8., ..., 8., 8., 8.]), array([9., 9., 9., ..., 9., 9., 9.]), array([10., 10., 10., ..., 10., 10., 10.]), array([11., 11., 11., ..., 11., 11., 11.]), array([12., 12., 12., ..., 12., 12., 12.]), array([13., 13., 13., ..., 13., 13., 13.]), array([14., 14., 14., ..., 14., 14., 14.]), array([15., 15., 15., ..., 15., 15., 15.]), array([16., 16., 16., ..., 16., 16., 16.]), array([17., 17., 17., ..., 17., 17., 17.]), array([18., 18., 18., ..., 18., 18., 18.]), array([19., 19., 19., ..., 19., 19., 19.]), array([20., 20., 20., ..., 20., 20., 20.]), array([21., 21., 21., ..., 21., 21., 21.]), array([22., 22., 22., ..., 22., 22., 22.]), array([23., 23., 23., ..., 23., 23., 23.]), array([24., 24., 24., ..., 24., 24., 24.]), array([25., 25., 25., ..., 25., 25., 25.]), array([26., 26., 26., ..., 26., 26., 26.]), array([27., 27., 27., ..., 27., 27., 27.]), array([28., 28., 28., ..., 28., 28., 28.]), array([29., 29., 29., ..., 29., 29., 29.]), array([30., 30., 30., ..., 30., 30., 30.]), array([31., 31., 31., ..., 31., 31., 31.]), array([32., 32., 32., ..., 32., 32., 32.]), array([33., 33., 33., ..., 33., 33., 33.]), array([34., 34., 34., ..., 34., 34., 34.])]\n",
            "2022\n",
            "4052\n",
            "2128\n",
            "3872\n",
            "3890\n",
            "3801\n",
            "3745\n",
            "3787\n",
            "1557\n",
            "2123\n",
            "3998\n",
            "2100\n",
            "3845\n",
            "3728\n",
            "1575\n",
            "3860\n",
            "3934\n",
            "3941\n",
            "4044\n",
            "2031\n",
            "4052\n",
            "1579\n",
            "3727\n",
            "3917\n",
            "2014\n",
            "3723\n",
            "1592\n",
            "2113\n",
            "3880\n",
            "1759\n",
            "3778\n",
            "3880\n",
            "2054\n",
            "2064\n",
            "1664\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten filename and y vectors\n",
        "filenames = [item for sublist in filenames for item in sublist]\n",
        "y = [item for sublist in y for item in sublist]"
      ],
      "metadata": {
        "id": "cwQw210SL0vw"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Associate filenames with true output and shuffle\n",
        "filenames_y = list(zip(filenames, y))\n",
        "random.shuffle(filenames_y)\n",
        "filenames, y = zip(*filenames_y)"
      ],
      "metadata": {
        "id": "IQGby27HMAlf"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Only keep the specified number of samples (shorter extraction/training)\n",
        "print(len(filenames))\n",
        "filenames = filenames[:int(len(filenames) * perc_keep_samples)]\n",
        "print(len(filenames))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0NjyrXaMZkV",
        "outputId": "b91a443c-ba6c-4d07-e335-d33125ff2473"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "105829\n",
            "105829\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate validation and test set sizes\n",
        "val_set_size = int(len(filenames) * val_ratio)\n",
        "test_set_size = int(len(filenames) * test_ratio)"
      ],
      "metadata": {
        "id": "uY4qsaFrMbrJ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Break dataset apart into train, validation, and test sets\n",
        "filenames_val = filenames[:val_set_size]\n",
        "filenames_test = filenames[val_set_size:(val_set_size + test_set_size)]\n",
        "filenames_train = filenames[(val_set_size + test_set_size):]"
      ],
      "metadata": {
        "id": "tmxa-HJRMeYy"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Break y apart into train, validation, and test sets\n",
        "y_orig_val = y[:val_set_size]\n",
        "y_orig_test = y[val_set_size:(val_set_size + test_set_size)]\n",
        "y_orig_train = y[(val_set_size + test_set_size):]"
      ],
      "metadata": {
        "id": "73WuKURbMjFw"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function: Create MFCC from given path\n",
        "def calc_mfcc(path):\n",
        "    \n",
        "    # Load wavefile\n",
        "    signal, fs = librosa.load(path, sr=sample_rate)\n",
        "    \n",
        "    # Create MFCCs from sound clip\n",
        "    mfccs = python_speech_features.base.mfcc(signal, \n",
        "                                            samplerate=fs,\n",
        "                                            winlen=0.256,\n",
        "                                            winstep=0.050,\n",
        "                                            numcep=num_mfcc,\n",
        "                                            nfilt=26,\n",
        "                                            nfft=2048,\n",
        "                                            preemph=1.0,\n",
        "                                            ceplifter=1,\n",
        "                                            appendEnergy=True,\n",
        "                                            winfunc=np.hanning)\n",
        "    return mfccs.transpose()"
      ],
      "metadata": {
        "id": "__joR3vvMki5"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST: Construct test set by computing MFCC of each WAV file\n",
        "prob_cnt = 0\n",
        "x_test = []\n",
        "y_test = []\n",
        "for index, filename in enumerate(filenames_train):\n",
        "    \n",
        "    # Stop after 500\n",
        "    if index >= 500:\n",
        "        break\n",
        "    \n",
        "    # Create path from given filename and target item\n",
        "    path = join(dataset_path, target_list[int(y_orig_train[index])], \n",
        "                filename)\n",
        "    \n",
        "    # Create MFCCs\n",
        "    mfccs = calc_mfcc(path)\n",
        "    \n",
        "    if mfccs.shape[1] == len_mfcc:\n",
        "        x_test.append(mfccs)\n",
        "        y_test.append(y_orig_train[index])\n",
        "    else:\n",
        "        print('Dropped:', index, mfccs.shape)\n",
        "        prob_cnt += 1\n",
        "        "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WM4hPRYrQRKi",
        "outputId": "a1fa19ce-d3de-4664-bd63-e61aa06cf61e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dropped: 35 (16, 15)\n",
            "Dropped: 39 (16, 13)\n",
            "Dropped: 43 (16, 12)\n",
            "Dropped: 50 (16, 14)\n",
            "Dropped: 58 (16, 11)\n",
            "Dropped: 62 (16, 13)\n",
            "Dropped: 67 (16, 14)\n",
            "Dropped: 74 (16, 8)\n",
            "Dropped: 100 (16, 12)\n",
            "Dropped: 101 (16, 14)\n",
            "Dropped: 107 (16, 8)\n",
            "Dropped: 115 (16, 11)\n",
            "Dropped: 117 (16, 12)\n",
            "Dropped: 122 (16, 13)\n",
            "Dropped: 148 (16, 14)\n",
            "Dropped: 158 (16, 15)\n",
            "Dropped: 202 (16, 13)\n",
            "Dropped: 212 (16, 7)\n",
            "Dropped: 216 (16, 14)\n",
            "Dropped: 228 (16, 11)\n",
            "Dropped: 236 (16, 12)\n",
            "Dropped: 245 (16, 11)\n",
            "Dropped: 254 (16, 14)\n",
            "Dropped: 284 (16, 12)\n",
            "Dropped: 287 (16, 11)\n",
            "Dropped: 290 (16, 8)\n",
            "Dropped: 298 (16, 12)\n",
            "Dropped: 300 (16, 15)\n",
            "Dropped: 301 (16, 10)\n",
            "Dropped: 315 (16, 14)\n",
            "Dropped: 322 (16, 10)\n",
            "Dropped: 328 (16, 11)\n",
            "Dropped: 335 (16, 10)\n",
            "Dropped: 336 (16, 15)\n",
            "Dropped: 345 (16, 15)\n",
            "Dropped: 348 (16, 15)\n",
            "Dropped: 355 (16, 10)\n",
            "Dropped: 370 (16, 15)\n",
            "Dropped: 373 (16, 11)\n",
            "Dropped: 380 (16, 7)\n",
            "Dropped: 386 (16, 10)\n",
            "Dropped: 390 (16, 13)\n",
            "Dropped: 395 (16, 12)\n",
            "Dropped: 415 (16, 13)\n",
            "Dropped: 429 (16, 9)\n",
            "Dropped: 446 (16, 14)\n",
            "Dropped: 464 (16, 13)\n",
            "Dropped: 472 (16, 13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('% of problematic samples:', prob_cnt / 500)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8V8BuuDEQjJg",
        "outputId": "05f15205-41ea-4b8d-a067-37c87bae4467"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "% of problematic samples: 0.096\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST: Test shorter MFCC\n",
        "#!pip install playsound\n",
        "#from playsound import playsound\n",
        "\n",
        "idx = 13\n",
        "\n",
        "# Create path from given filename and target item\n",
        "path = join(dataset_path, target_list[int(y_orig_train[idx])], \n",
        "            filenames_train[idx])\n",
        "\n",
        "# Create MFCCs\n",
        "mfccs = calc_mfcc(path)\n",
        "print(\"MFCCs:\", mfccs)\n",
        "\n",
        "# Plot MFCC\n",
        "fig = plt.figure()\n",
        "plt.imshow(mfccs, cmap='inferno', origin='lower')\n",
        "\n",
        "# TEST: Play problem sounds\n",
        "print(target_list[int(y_orig_train[idx])])\n",
        "#playsound(path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AZ2VC14FQp4q",
        "outputId": "5a06b450-7b50-4793-8e14-84389f3b2bed"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MFCCs: [[-2.76518903e+00 -2.83573721e+00 -2.91722709e+00 -2.84243276e+00\n",
            "  -2.76972111e+00 -2.87873492e+00 -2.85576131e+00 -2.70922787e+00\n",
            "  -2.73009829e+00 -2.75923634e+00 -2.75577492e+00 -2.12215723e+00\n",
            "   5.02327895e-01  2.27613836e+00  2.99553584e+00  2.82209662e+00]\n",
            " [-1.34784637e-01  6.71150665e-02 -3.35987809e-01 -3.60584691e-01\n",
            "  -1.04734800e-01 -9.09240380e-02 -3.03453168e-01 -1.89984162e-01\n",
            "  -4.49826007e-01 -1.15516191e+00 -1.38927494e+00 -1.60476110e+00\n",
            "  -4.96031323e+00 -7.73812796e+00 -8.02020064e+00 -6.47587528e+00]\n",
            " [-1.27613236e+00 -1.07551870e+00 -1.37728678e+00 -1.69393183e+00\n",
            "  -1.72338500e+00 -1.65216022e+00 -1.39768747e+00 -9.84856020e-01\n",
            "  -1.24476769e+00 -1.92498793e+00 -2.31210715e+00 -3.40996318e+00\n",
            "  -6.27036644e+00 -7.51618750e+00 -7.95806918e+00 -7.84683947e+00]\n",
            " [ 1.27172942e+00  1.36153957e+00  9.40553972e-01  1.02695739e+00\n",
            "   1.15351267e+00  9.25644766e-01  8.97588232e-01  1.19673132e+00\n",
            "   1.14607017e+00  5.65317660e-01  1.54176947e-01 -3.25125329e-01\n",
            "  -1.51405860e+00 -2.79397533e+00 -3.59464264e+00 -4.43987425e+00]\n",
            " [-3.10329994e-01 -9.14480717e-01 -1.52712884e+00 -9.88494114e-01\n",
            "  -2.68646010e-01 -3.55375098e-01 -7.89395839e-01 -7.62806741e-01\n",
            "  -9.62022018e-01 -1.64368276e+00 -1.96885445e+00 -5.38771970e-02\n",
            "   9.85063451e-01  1.38772559e+00  1.87512711e+00  2.08079567e+00]\n",
            " [ 7.86043323e-01  1.02629617e+00  4.34667121e-01  3.58137900e-01\n",
            "   7.31358021e-01  4.35206547e-01  1.72322825e-01  4.09785712e-01\n",
            "   2.40624680e-01 -4.26868939e-01 -5.56844632e-01  8.69253106e-01\n",
            "   2.06258407e+00  2.68858999e+00  2.97518655e+00  2.54964848e+00]\n",
            " [-3.23509053e-01 -3.52640774e-01 -5.66771561e-01 -4.94881507e-01\n",
            "  -5.51078262e-01 -8.40175987e-01 -4.88238789e-01 -1.48498684e-01\n",
            "  -3.46395522e-01 -9.69113388e-01 -1.16350580e+00 -1.67048166e+00\n",
            "  -2.34559835e+00 -2.04452032e+00 -1.58297273e+00 -1.47627172e+00]\n",
            " [ 4.70419998e-01  4.37480606e-01  5.91415647e-01  6.08685449e-01\n",
            "   3.97295514e-01  1.72888692e-01  3.61502766e-01  8.75224904e-01\n",
            "   1.07669598e+00  3.64425314e-01 -3.31199819e-02 -3.63154520e-01\n",
            "  -2.99059374e-01 -3.72457998e-01 -6.10775995e-02  4.19155444e-01]\n",
            " [-4.86651496e-01 -4.45925699e-01 -2.67383695e-01 -7.42755594e-02\n",
            "  -2.08892757e-01 -5.04079940e-01 -4.39284537e-01 -2.91312252e-01\n",
            "  -3.48123519e-01 -9.23587072e-01 -1.50040502e+00 -1.25730018e+00\n",
            "  -1.58861386e+00 -1.80553966e+00 -2.08942018e+00 -2.58723179e+00]\n",
            " [-1.66216045e-01 -2.65065845e-01 -3.21306712e-01 -5.40479528e-02\n",
            "  -1.55159874e-01 -2.59255431e-01 -3.34583613e-01 -3.69328801e-01\n",
            "  -1.99078151e-01 -4.48408401e-01 -8.41333748e-01  3.14315776e-01\n",
            "   6.73457827e-01  1.68175015e-02 -2.80467283e-01 -5.23536209e-01]\n",
            " [-8.88421359e-01 -6.32170640e-01 -9.54902910e-01 -9.04747606e-01\n",
            "  -7.82937384e-01 -8.02237539e-01 -7.58771445e-01 -6.67642101e-01\n",
            "  -5.91855296e-01 -5.73609464e-01 -6.71730354e-01 -8.14817186e-01\n",
            "  -1.54977284e+00 -1.84821913e+00 -1.30954289e+00 -3.77818699e-01]\n",
            " [-4.17719382e-01 -1.08913316e-01 -2.39323115e-02 -8.82332695e-02\n",
            "  -1.34399638e-01  3.14235908e-02  2.76424156e-01  2.91562207e-01\n",
            "   1.00826823e-01 -4.56635451e-02 -1.24144490e-01 -3.04845072e-01\n",
            "  -8.42814246e-02 -4.46073874e-01 -5.70218435e-01 -2.39001409e-01]\n",
            " [-5.83727618e-01 -7.07862253e-01 -6.21612129e-01 -5.48033525e-01\n",
            "  -3.83665729e-01 -3.53857427e-01 -4.94023387e-01 -2.35298916e-01\n",
            "  -2.33003887e-01 -6.91210788e-01 -8.88981420e-01 -1.37740223e-01\n",
            "   5.09213263e-01  3.84154404e-01  2.09800711e-01  1.43916310e-01]\n",
            " [-5.07698652e-02 -1.72805195e-01  7.94361323e-02  5.04431813e-01\n",
            "   6.85882640e-01  5.37186819e-01  2.62415353e-01  2.48696413e-01\n",
            "   4.71292147e-01  3.56097451e-01  1.56063494e-01  4.21621047e-01\n",
            "   4.10943690e-02 -8.26078740e-02 -2.85963335e-01 -3.34872668e-01]\n",
            " [-3.71949112e-01 -6.91562968e-01 -6.05409464e-01 -2.99402488e-01\n",
            "  -1.19296337e-01  1.64001127e-01  3.09282329e-01  7.82715144e-02\n",
            "  -7.36828555e-02 -1.89782562e-01 -2.73836722e-01 -1.21109108e-01\n",
            "  -6.12313485e-02  1.07772831e-02 -3.18779188e-01 -7.50955779e-01]\n",
            " [-1.76620981e-01 -1.03614707e-01  1.19118463e-01  1.67760591e-01\n",
            "   3.81916307e-02  5.86203047e-02  3.53102816e-01  4.25221448e-01\n",
            "   2.72611277e-01  2.50688697e-01  1.98302839e-01 -3.30286613e-01\n",
            "  -5.12308523e-03  3.54946709e-01  6.58781542e-02 -5.35456082e-01]]\n",
            "house\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function: Create MFCCs, keeping only ones of desired length\n",
        "def extract_features(in_files, in_y):\n",
        "    prob_cnt = 0\n",
        "    out_x = []\n",
        "    out_y = []\n",
        "        \n",
        "    for index, filename in enumerate(in_files):\n",
        "    \n",
        "        # Create path from given filename and target item\n",
        "        path = join(dataset_path, target_list[int(in_y[index])], \n",
        "                    filename)\n",
        "        \n",
        "        # Check to make sure we're reading a .wav file\n",
        "        if not path.endswith('.wav'):\n",
        "            continue\n",
        "\n",
        "        # Create MFCCs\n",
        "        mfccs = calc_mfcc(path)\n",
        "\n",
        "        # Only keep MFCCs with given length\n",
        "        if mfccs.shape[1] == len_mfcc:\n",
        "            out_x.append(mfccs)\n",
        "            out_y.append(in_y[index])\n",
        "        else:\n",
        "            print('Dropped:', index, mfccs.shape)\n",
        "            prob_cnt += 1\n",
        "            \n",
        "    return out_x, out_y, prob_cnt"
      ],
      "metadata": {
        "id": "p_R1dDLlSj3K"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create train, validation, and test sets\n",
        "x_train, y_train, prob = extract_features(filenames_train, \n",
        "                                          y_orig_train)\n",
        "print('Removed percentage:', prob / len(y_orig_train))\n",
        "x_val, y_val, prob = extract_features(filenames_val, y_orig_val)\n",
        "print('Removed percentage:', prob / len(y_orig_val))\n",
        "x_test, y_test, prob = extract_features(filenames_test, y_orig_test)\n",
        "print('Removed percentage:', prob / len(y_orig_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wtk1BMn9Sqpl",
        "outputId": "e9ce0c35-fc27-4a10-c59e-ff0c3f30b27d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dropped: 35 (16, 15)\n",
            "Dropped: 39 (16, 13)\n",
            "Dropped: 43 (16, 12)\n",
            "Dropped: 50 (16, 14)\n",
            "Dropped: 58 (16, 11)\n",
            "Dropped: 62 (16, 13)\n",
            "Dropped: 67 (16, 14)\n",
            "Dropped: 74 (16, 8)\n",
            "Dropped: 100 (16, 12)\n",
            "Dropped: 101 (16, 14)\n",
            "Dropped: 107 (16, 8)\n",
            "Dropped: 115 (16, 11)\n",
            "Dropped: 117 (16, 12)\n",
            "Dropped: 122 (16, 13)\n",
            "Dropped: 148 (16, 14)\n",
            "Dropped: 158 (16, 15)\n",
            "Dropped: 202 (16, 13)\n",
            "Dropped: 212 (16, 7)\n",
            "Dropped: 216 (16, 14)\n",
            "Dropped: 228 (16, 11)\n",
            "Dropped: 236 (16, 12)\n",
            "Dropped: 245 (16, 11)\n",
            "Dropped: 254 (16, 14)\n",
            "Dropped: 284 (16, 12)\n",
            "Dropped: 287 (16, 11)\n",
            "Dropped: 290 (16, 8)\n",
            "Dropped: 298 (16, 12)\n",
            "Dropped: 300 (16, 15)\n",
            "Dropped: 301 (16, 10)\n",
            "Dropped: 315 (16, 14)\n",
            "Dropped: 322 (16, 10)\n",
            "Dropped: 328 (16, 11)\n",
            "Dropped: 335 (16, 10)\n",
            "Dropped: 336 (16, 15)\n",
            "Dropped: 345 (16, 15)\n",
            "Dropped: 348 (16, 15)\n",
            "Dropped: 355 (16, 10)\n",
            "Dropped: 370 (16, 15)\n",
            "Dropped: 373 (16, 11)\n",
            "Dropped: 380 (16, 7)\n",
            "Dropped: 386 (16, 10)\n",
            "Dropped: 390 (16, 13)\n",
            "Dropped: 395 (16, 12)\n",
            "Dropped: 415 (16, 13)\n",
            "Dropped: 429 (16, 9)\n",
            "Dropped: 446 (16, 14)\n",
            "Dropped: 464 (16, 13)\n",
            "Dropped: 472 (16, 13)\n",
            "Dropped: 506 (16, 12)\n",
            "Dropped: 532 (16, 9)\n",
            "Dropped: 544 (16, 12)\n",
            "Dropped: 555 (16, 8)\n",
            "Dropped: 557 (16, 14)\n",
            "Dropped: 564 (16, 11)\n",
            "Dropped: 576 (16, 11)\n",
            "Dropped: 591 (16, 12)\n",
            "Dropped: 607 (16, 9)\n",
            "Dropped: 618 (16, 11)\n",
            "Dropped: 639 (16, 14)\n",
            "Dropped: 653 (16, 12)\n",
            "Dropped: 660 (16, 11)\n",
            "Dropped: 722 (16, 14)\n",
            "Dropped: 765 (16, 13)\n",
            "Dropped: 791 (16, 10)\n",
            "Dropped: 799 (16, 12)\n",
            "Dropped: 809 (16, 13)\n",
            "Dropped: 839 (16, 15)\n",
            "Dropped: 853 (16, 14)\n",
            "Dropped: 854 (16, 10)\n",
            "Dropped: 857 (16, 15)\n",
            "Dropped: 865 (16, 7)\n",
            "Dropped: 868 (16, 3)\n",
            "Dropped: 870 (16, 14)\n",
            "Dropped: 874 (16, 11)\n",
            "Dropped: 875 (16, 9)\n",
            "Dropped: 878 (16, 15)\n",
            "Dropped: 882 (16, 6)\n",
            "Dropped: 891 (16, 13)\n",
            "Dropped: 900 (16, 7)\n",
            "Dropped: 912 (16, 15)\n",
            "Dropped: 920 (16, 15)\n",
            "Dropped: 927 (16, 15)\n",
            "Dropped: 929 (16, 14)\n",
            "Dropped: 933 (16, 14)\n",
            "Dropped: 952 (16, 15)\n",
            "Dropped: 988 (16, 12)\n",
            "Dropped: 989 (16, 12)\n",
            "Dropped: 991 (16, 13)\n",
            "Dropped: 995 (16, 9)\n",
            "Dropped: 1006 (16, 13)\n",
            "Dropped: 1020 (16, 15)\n",
            "Dropped: 1042 (16, 13)\n",
            "Dropped: 1047 (16, 13)\n",
            "Dropped: 1069 (16, 13)\n",
            "Dropped: 1081 (16, 7)\n",
            "Dropped: 1096 (16, 15)\n",
            "Dropped: 1117 (16, 12)\n",
            "Dropped: 1119 (16, 15)\n",
            "Dropped: 1121 (16, 14)\n",
            "Dropped: 1145 (16, 12)\n",
            "Dropped: 1155 (16, 15)\n",
            "Dropped: 1170 (16, 11)\n",
            "Dropped: 1174 (16, 14)\n",
            "Dropped: 1179 (16, 13)\n",
            "Dropped: 1192 (16, 9)\n",
            "Dropped: 1204 (16, 15)\n",
            "Dropped: 1205 (16, 12)\n",
            "Dropped: 1213 (16, 15)\n",
            "Dropped: 1223 (16, 14)\n",
            "Dropped: 1254 (16, 11)\n",
            "Dropped: 1295 (16, 10)\n",
            "Dropped: 1304 (16, 13)\n",
            "Dropped: 1307 (16, 14)\n",
            "Dropped: 1335 (16, 10)\n",
            "Dropped: 1345 (16, 12)\n",
            "Dropped: 1349 (16, 15)\n",
            "Dropped: 1352 (16, 14)\n",
            "Dropped: 1360 (16, 7)\n",
            "Dropped: 1388 (16, 11)\n",
            "Dropped: 1396 (16, 15)\n",
            "Dropped: 1406 (16, 8)\n",
            "Dropped: 1419 (16, 12)\n",
            "Dropped: 1423 (16, 9)\n",
            "Dropped: 1444 (16, 8)\n",
            "Dropped: 1454 (16, 12)\n",
            "Dropped: 1466 (16, 15)\n",
            "Dropped: 1471 (16, 7)\n",
            "Dropped: 1476 (16, 12)\n",
            "Dropped: 1495 (16, 11)\n",
            "Dropped: 1496 (16, 11)\n",
            "Dropped: 1501 (16, 10)\n",
            "Dropped: 1513 (16, 5)\n",
            "Dropped: 1516 (16, 15)\n",
            "Dropped: 1532 (16, 12)\n",
            "Dropped: 1545 (16, 10)\n",
            "Dropped: 1549 (16, 15)\n",
            "Dropped: 1558 (16, 15)\n",
            "Dropped: 1560 (16, 13)\n",
            "Dropped: 1562 (16, 12)\n",
            "Dropped: 1583 (16, 12)\n",
            "Dropped: 1602 (16, 11)\n",
            "Dropped: 1609 (16, 10)\n",
            "Dropped: 1612 (16, 11)\n",
            "Dropped: 1653 (16, 14)\n",
            "Dropped: 1672 (16, 13)\n",
            "Dropped: 1675 (16, 12)\n",
            "Dropped: 1707 (16, 15)\n",
            "Dropped: 1709 (16, 12)\n",
            "Dropped: 1712 (16, 13)\n",
            "Dropped: 1713 (16, 13)\n",
            "Dropped: 1734 (16, 11)\n",
            "Dropped: 1735 (16, 11)\n",
            "Dropped: 1736 (16, 14)\n",
            "Dropped: 1738 (16, 5)\n",
            "Dropped: 1793 (16, 14)\n",
            "Dropped: 1812 (16, 14)\n",
            "Dropped: 1821 (16, 8)\n",
            "Dropped: 1828 (16, 15)\n",
            "Dropped: 1849 (16, 13)\n",
            "Dropped: 1857 (16, 15)\n",
            "Dropped: 1862 (16, 7)\n",
            "Dropped: 1863 (16, 10)\n",
            "Dropped: 1864 (16, 13)\n",
            "Dropped: 1865 (16, 7)\n",
            "Dropped: 1868 (16, 8)\n",
            "Dropped: 1871 (16, 13)\n",
            "Dropped: 1883 (16, 15)\n",
            "Dropped: 1906 (16, 10)\n",
            "Dropped: 1912 (16, 10)\n",
            "Dropped: 1922 (16, 13)\n",
            "Dropped: 1937 (16, 11)\n",
            "Dropped: 1966 (16, 15)\n",
            "Dropped: 1979 (16, 15)\n",
            "Dropped: 2023 (16, 14)\n",
            "Dropped: 2052 (16, 7)\n",
            "Dropped: 2059 (16, 15)\n",
            "Dropped: 2065 (16, 13)\n",
            "Dropped: 2079 (16, 13)\n",
            "Dropped: 2084 (16, 13)\n",
            "Dropped: 2135 (16, 15)\n",
            "Dropped: 2137 (16, 8)\n",
            "Dropped: 2165 (16, 13)\n",
            "Dropped: 2189 (16, 15)\n",
            "Dropped: 2206 (16, 14)\n",
            "Dropped: 2211 (16, 13)\n",
            "Dropped: 2213 (16, 11)\n",
            "Dropped: 2225 (16, 11)\n",
            "Dropped: 2238 (16, 15)\n",
            "Dropped: 2249 (16, 11)\n",
            "Dropped: 2252 (16, 7)\n",
            "Dropped: 2266 (16, 13)\n",
            "Dropped: 2267 (16, 12)\n",
            "Dropped: 2269 (16, 12)\n",
            "Dropped: 2284 (16, 7)\n",
            "Dropped: 2296 (16, 14)\n",
            "Dropped: 2310 (16, 9)\n",
            "Dropped: 2336 (16, 15)\n",
            "Dropped: 2340 (16, 13)\n",
            "Dropped: 2344 (16, 12)\n",
            "Dropped: 2349 (16, 14)\n",
            "Dropped: 2353 (16, 15)\n",
            "Dropped: 2360 (16, 13)\n",
            "Dropped: 2375 (16, 11)\n",
            "Dropped: 2376 (16, 10)\n",
            "Dropped: 2391 (16, 13)\n",
            "Dropped: 2397 (16, 15)\n",
            "Dropped: 2405 (16, 7)\n",
            "Dropped: 2411 (16, 13)\n",
            "Dropped: 2413 (16, 12)\n",
            "Dropped: 2418 (16, 11)\n",
            "Dropped: 2441 (16, 15)\n",
            "Dropped: 2454 (16, 12)\n",
            "Dropped: 2455 (16, 11)\n",
            "Dropped: 2471 (16, 14)\n",
            "Dropped: 2474 (16, 13)\n",
            "Dropped: 2487 (16, 15)\n",
            "Dropped: 2496 (16, 15)\n",
            "Dropped: 2498 (16, 9)\n",
            "Dropped: 2500 (16, 11)\n",
            "Dropped: 2521 (16, 14)\n",
            "Dropped: 2545 (16, 13)\n",
            "Dropped: 2565 (16, 9)\n",
            "Dropped: 2589 (16, 11)\n",
            "Dropped: 2617 (16, 13)\n",
            "Dropped: 2646 (16, 13)\n",
            "Dropped: 2651 (16, 9)\n",
            "Dropped: 2655 (16, 15)\n",
            "Dropped: 2659 (16, 8)\n",
            "Dropped: 2662 (16, 10)\n",
            "Dropped: 2682 (16, 15)\n",
            "Dropped: 2685 (16, 9)\n",
            "Dropped: 2695 (16, 14)\n",
            "Dropped: 2696 (16, 15)\n",
            "Dropped: 2728 (16, 12)\n",
            "Dropped: 2730 (16, 13)\n",
            "Dropped: 2732 (16, 10)\n",
            "Dropped: 2750 (16, 6)\n",
            "Dropped: 2765 (16, 15)\n",
            "Dropped: 2776 (16, 10)\n",
            "Dropped: 2791 (16, 15)\n",
            "Dropped: 2800 (16, 13)\n",
            "Dropped: 2810 (16, 9)\n",
            "Dropped: 2812 (16, 12)\n",
            "Dropped: 2814 (16, 14)\n",
            "Dropped: 2841 (16, 11)\n",
            "Dropped: 2857 (16, 12)\n",
            "Dropped: 2868 (16, 11)\n",
            "Dropped: 2893 (16, 15)\n",
            "Dropped: 2907 (16, 14)\n",
            "Dropped: 2921 (16, 11)\n",
            "Dropped: 2942 (16, 9)\n",
            "Dropped: 2945 (16, 13)\n",
            "Dropped: 2961 (16, 11)\n",
            "Dropped: 2962 (16, 11)\n",
            "Dropped: 2969 (16, 10)\n",
            "Dropped: 2979 (16, 14)\n",
            "Dropped: 2996 (16, 13)\n",
            "Dropped: 2998 (16, 15)\n",
            "Dropped: 2999 (16, 11)\n",
            "Dropped: 3010 (16, 15)\n",
            "Dropped: 3020 (16, 13)\n",
            "Dropped: 3034 (16, 8)\n",
            "Dropped: 3037 (16, 10)\n",
            "Dropped: 3061 (16, 15)\n",
            "Dropped: 3062 (16, 12)\n",
            "Dropped: 3067 (16, 14)\n",
            "Dropped: 3138 (16, 13)\n",
            "Dropped: 3144 (16, 11)\n",
            "Dropped: 3153 (16, 13)\n",
            "Dropped: 3157 (16, 7)\n",
            "Dropped: 3161 (16, 14)\n",
            "Dropped: 3163 (16, 15)\n",
            "Dropped: 3164 (16, 15)\n",
            "Dropped: 3169 (16, 14)\n",
            "Dropped: 3187 (16, 13)\n",
            "Dropped: 3194 (16, 13)\n",
            "Dropped: 3198 (16, 14)\n",
            "Dropped: 3225 (16, 13)\n",
            "Dropped: 3271 (16, 9)\n",
            "Dropped: 3279 (16, 13)\n",
            "Dropped: 3288 (16, 10)\n",
            "Dropped: 3290 (16, 11)\n",
            "Dropped: 3302 (16, 12)\n",
            "Dropped: 3321 (16, 11)\n",
            "Dropped: 3347 (16, 15)\n",
            "Dropped: 3348 (16, 13)\n",
            "Dropped: 3353 (16, 12)\n",
            "Dropped: 3369 (16, 12)\n",
            "Dropped: 3376 (16, 13)\n",
            "Dropped: 3382 (16, 13)\n",
            "Dropped: 3394 (16, 14)\n",
            "Dropped: 3406 (16, 15)\n",
            "Dropped: 3433 (16, 7)\n",
            "Dropped: 3446 (16, 11)\n",
            "Dropped: 3452 (16, 13)\n",
            "Dropped: 3482 (16, 14)\n",
            "Dropped: 3495 (16, 15)\n",
            "Dropped: 3519 (16, 11)\n",
            "Dropped: 3530 (16, 13)\n",
            "Dropped: 3606 (16, 14)\n",
            "Dropped: 3609 (16, 9)\n",
            "Dropped: 3621 (16, 15)\n",
            "Dropped: 3625 (16, 11)\n",
            "Dropped: 3630 (16, 11)\n",
            "Dropped: 3645 (16, 9)\n",
            "Dropped: 3654 (16, 11)\n",
            "Dropped: 3656 (16, 14)\n",
            "Dropped: 3661 (16, 13)\n",
            "Dropped: 3669 (16, 13)\n",
            "Dropped: 3675 (16, 13)\n",
            "Dropped: 3679 (16, 12)\n",
            "Dropped: 3684 (16, 9)\n",
            "Dropped: 3703 (16, 11)\n",
            "Dropped: 3713 (16, 11)\n",
            "Dropped: 3726 (16, 14)\n",
            "Dropped: 3744 (16, 13)\n",
            "Dropped: 3754 (16, 15)\n",
            "Dropped: 3792 (16, 7)\n",
            "Dropped: 3800 (16, 15)\n",
            "Dropped: 3821 (16, 11)\n",
            "Dropped: 3825 (16, 9)\n",
            "Dropped: 3834 (16, 13)\n",
            "Dropped: 3868 (16, 14)\n",
            "Dropped: 3870 (16, 14)\n",
            "Dropped: 3873 (16, 13)\n",
            "Dropped: 3877 (16, 13)\n",
            "Dropped: 3904 (16, 8)\n",
            "Dropped: 3915 (16, 12)\n",
            "Dropped: 3916 (16, 5)\n",
            "Dropped: 3926 (16, 14)\n",
            "Dropped: 3934 (16, 13)\n",
            "Dropped: 3952 (16, 11)\n",
            "Dropped: 3957 (16, 15)\n",
            "Dropped: 3965 (16, 11)\n",
            "Dropped: 3978 (16, 8)\n",
            "Dropped: 3982 (16, 6)\n",
            "Dropped: 4021 (16, 5)\n",
            "Dropped: 4038 (16, 15)\n",
            "Dropped: 4092 (16, 11)\n",
            "Dropped: 4103 (16, 10)\n",
            "Dropped: 4109 (16, 14)\n",
            "Dropped: 4115 (16, 11)\n",
            "Dropped: 4116 (16, 11)\n",
            "Dropped: 4117 (16, 11)\n",
            "Dropped: 4133 (16, 11)\n",
            "Dropped: 4152 (16, 8)\n",
            "Dropped: 4167 (16, 15)\n",
            "Dropped: 4210 (16, 14)\n",
            "Dropped: 4234 (16, 12)\n",
            "Dropped: 4237 (16, 10)\n",
            "Dropped: 4252 (16, 14)\n",
            "Dropped: 4262 (16, 11)\n",
            "Dropped: 4271 (16, 14)\n",
            "Dropped: 4275 (16, 7)\n",
            "Dropped: 4278 (16, 8)\n",
            "Dropped: 4282 (16, 10)\n",
            "Dropped: 4285 (16, 15)\n",
            "Dropped: 4295 (16, 12)\n",
            "Dropped: 4304 (16, 12)\n",
            "Dropped: 4305 (16, 11)\n",
            "Dropped: 4315 (16, 13)\n",
            "Dropped: 4323 (16, 14)\n",
            "Dropped: 4327 (16, 15)\n",
            "Dropped: 4329 (16, 13)\n",
            "Dropped: 4335 (16, 13)\n",
            "Dropped: 4341 (16, 15)\n",
            "Dropped: 4367 (16, 14)\n",
            "Dropped: 4372 (16, 11)\n",
            "Dropped: 4374 (16, 11)\n",
            "Dropped: 4375 (16, 15)\n",
            "Dropped: 4376 (16, 11)\n",
            "Dropped: 4402 (16, 11)\n",
            "Dropped: 4405 (16, 11)\n",
            "Dropped: 4412 (16, 8)\n",
            "Dropped: 4413 (16, 13)\n",
            "Dropped: 4415 (16, 11)\n",
            "Dropped: 4420 (16, 10)\n",
            "Dropped: 4426 (16, 12)\n",
            "Dropped: 4432 (16, 14)\n",
            "Dropped: 4443 (16, 8)\n",
            "Dropped: 4446 (16, 9)\n",
            "Dropped: 4464 (16, 12)\n",
            "Dropped: 4476 (16, 11)\n",
            "Dropped: 4498 (16, 15)\n",
            "Dropped: 4504 (16, 10)\n",
            "Dropped: 4538 (16, 12)\n",
            "Dropped: 4563 (16, 12)\n",
            "Dropped: 4576 (16, 12)\n",
            "Dropped: 4587 (16, 11)\n",
            "Dropped: 4593 (16, 11)\n",
            "Dropped: 4599 (16, 9)\n",
            "Dropped: 4614 (16, 13)\n",
            "Dropped: 4625 (16, 10)\n",
            "Dropped: 4639 (16, 13)\n",
            "Dropped: 4645 (16, 2)\n",
            "Dropped: 4660 (16, 12)\n",
            "Dropped: 4679 (16, 13)\n",
            "Dropped: 4681 (16, 15)\n",
            "Dropped: 4683 (16, 14)\n",
            "Dropped: 4685 (16, 5)\n",
            "Dropped: 4690 (16, 15)\n",
            "Dropped: 4693 (16, 14)\n",
            "Dropped: 4697 (16, 15)\n",
            "Dropped: 4699 (16, 13)\n",
            "Dropped: 4700 (16, 13)\n",
            "Dropped: 4703 (16, 7)\n",
            "Dropped: 4710 (16, 14)\n",
            "Dropped: 4727 (16, 13)\n",
            "Dropped: 4732 (16, 7)\n",
            "Dropped: 4745 (16, 12)\n",
            "Dropped: 4746 (16, 10)\n",
            "Dropped: 4780 (16, 11)\n",
            "Dropped: 4797 (16, 13)\n",
            "Dropped: 4800 (16, 11)\n",
            "Dropped: 4801 (16, 6)\n",
            "Dropped: 4818 (16, 12)\n",
            "Dropped: 4824 (16, 13)\n",
            "Dropped: 4826 (16, 8)\n",
            "Dropped: 4831 (16, 11)\n",
            "Dropped: 4834 (16, 11)\n",
            "Dropped: 4844 (16, 10)\n",
            "Dropped: 4848 (16, 12)\n",
            "Dropped: 4855 (16, 15)\n",
            "Dropped: 4857 (16, 13)\n",
            "Dropped: 4868 (16, 12)\n",
            "Dropped: 4869 (16, 7)\n",
            "Dropped: 4874 (16, 9)\n",
            "Dropped: 4878 (16, 9)\n",
            "Dropped: 4903 (16, 12)\n",
            "Dropped: 4913 (16, 8)\n",
            "Dropped: 4920 (16, 11)\n",
            "Dropped: 4923 (16, 12)\n",
            "Dropped: 4927 (16, 11)\n",
            "Dropped: 4936 (16, 15)\n",
            "Dropped: 4956 (16, 12)\n",
            "Dropped: 4958 (16, 12)\n",
            "Dropped: 4965 (16, 15)\n",
            "Dropped: 4971 (16, 13)\n",
            "Dropped: 5013 (16, 13)\n",
            "Dropped: 5032 (16, 12)\n",
            "Dropped: 5051 (16, 15)\n",
            "Dropped: 5053 (16, 8)\n",
            "Dropped: 5096 (16, 14)\n",
            "Dropped: 5103 (16, 14)\n",
            "Dropped: 5104 (16, 10)\n",
            "Dropped: 5109 (16, 13)\n",
            "Dropped: 5115 (16, 10)\n",
            "Dropped: 5130 (16, 13)\n",
            "Dropped: 5171 (16, 8)\n",
            "Dropped: 5185 (16, 12)\n",
            "Dropped: 5191 (16, 10)\n",
            "Dropped: 5197 (16, 7)\n",
            "Dropped: 5223 (16, 7)\n",
            "Dropped: 5261 (16, 8)\n",
            "Dropped: 5262 (16, 14)\n",
            "Dropped: 5285 (16, 14)\n",
            "Dropped: 5290 (16, 11)\n",
            "Dropped: 5293 (16, 11)\n",
            "Dropped: 5299 (16, 14)\n",
            "Dropped: 5309 (16, 11)\n",
            "Dropped: 5345 (16, 11)\n",
            "Dropped: 5357 (16, 14)\n",
            "Dropped: 5358 (16, 15)\n",
            "Dropped: 5362 (16, 15)\n",
            "Dropped: 5363 (16, 15)\n",
            "Dropped: 5397 (16, 10)\n",
            "Dropped: 5409 (16, 10)\n",
            "Dropped: 5424 (16, 11)\n",
            "Dropped: 5463 (16, 12)\n",
            "Dropped: 5464 (16, 11)\n",
            "Dropped: 5473 (16, 15)\n",
            "Dropped: 5482 (16, 10)\n",
            "Dropped: 5483 (16, 13)\n",
            "Dropped: 5486 (16, 8)\n",
            "Dropped: 5526 (16, 10)\n",
            "Dropped: 5528 (16, 9)\n",
            "Dropped: 5562 (16, 11)\n",
            "Dropped: 5566 (16, 11)\n",
            "Dropped: 5577 (16, 13)\n",
            "Dropped: 5609 (16, 12)\n",
            "Dropped: 5610 (16, 3)\n",
            "Dropped: 5613 (16, 12)\n",
            "Dropped: 5631 (16, 11)\n",
            "Dropped: 5658 (16, 11)\n",
            "Dropped: 5666 (16, 12)\n",
            "Dropped: 5683 (16, 14)\n",
            "Dropped: 5691 (16, 12)\n",
            "Dropped: 5708 (16, 15)\n",
            "Dropped: 5711 (16, 13)\n",
            "Dropped: 5719 (16, 12)\n",
            "Dropped: 5722 (16, 14)\n",
            "Dropped: 5728 (16, 12)\n",
            "Dropped: 5729 (16, 15)\n",
            "Dropped: 5745 (16, 15)\n",
            "Dropped: 5748 (16, 13)\n",
            "Dropped: 5754 (16, 15)\n",
            "Dropped: 5759 (16, 12)\n",
            "Dropped: 5778 (16, 14)\n",
            "Dropped: 5780 (16, 13)\n",
            "Dropped: 5803 (16, 10)\n",
            "Dropped: 5819 (16, 14)\n",
            "Dropped: 5837 (16, 5)\n",
            "Dropped: 5841 (16, 8)\n",
            "Dropped: 5846 (16, 15)\n",
            "Dropped: 5853 (16, 13)\n",
            "Dropped: 5862 (16, 11)\n",
            "Dropped: 5875 (16, 13)\n",
            "Dropped: 5883 (16, 13)\n",
            "Dropped: 5899 (16, 8)\n",
            "Dropped: 5904 (16, 15)\n",
            "Dropped: 5917 (16, 11)\n",
            "Dropped: 5920 (16, 13)\n",
            "Dropped: 5922 (16, 14)\n",
            "Dropped: 5924 (16, 15)\n",
            "Dropped: 5940 (16, 12)\n",
            "Dropped: 5942 (16, 11)\n",
            "Dropped: 5946 (16, 9)\n",
            "Dropped: 5992 (16, 13)\n",
            "Dropped: 5995 (16, 13)\n",
            "Dropped: 6029 (16, 13)\n",
            "Dropped: 6030 (16, 14)\n",
            "Dropped: 6057 (16, 13)\n",
            "Dropped: 6069 (16, 15)\n",
            "Dropped: 6081 (16, 12)\n",
            "Dropped: 6089 (16, 15)\n",
            "Dropped: 6090 (16, 14)\n",
            "Dropped: 6094 (16, 13)\n",
            "Dropped: 6100 (16, 12)\n",
            "Dropped: 6101 (16, 14)\n",
            "Dropped: 6114 (16, 9)\n",
            "Dropped: 6123 (16, 8)\n",
            "Dropped: 6129 (16, 15)\n",
            "Dropped: 6145 (16, 14)\n",
            "Dropped: 6153 (16, 11)\n",
            "Dropped: 6163 (16, 13)\n",
            "Dropped: 6165 (16, 11)\n",
            "Dropped: 6184 (16, 12)\n",
            "Dropped: 6190 (16, 14)\n",
            "Dropped: 6198 (16, 15)\n",
            "Dropped: 6223 (16, 11)\n",
            "Dropped: 6227 (16, 12)\n",
            "Dropped: 6235 (16, 11)\n",
            "Dropped: 6237 (16, 11)\n",
            "Dropped: 6250 (16, 13)\n",
            "Dropped: 6254 (16, 9)\n",
            "Dropped: 6260 (16, 10)\n",
            "Dropped: 6265 (16, 15)\n",
            "Dropped: 6275 (16, 4)\n",
            "Dropped: 6284 (16, 13)\n",
            "Dropped: 6316 (16, 13)\n",
            "Dropped: 6333 (16, 15)\n",
            "Dropped: 6337 (16, 8)\n",
            "Dropped: 6346 (16, 5)\n",
            "Dropped: 6359 (16, 14)\n",
            "Dropped: 6372 (16, 14)\n",
            "Dropped: 6379 (16, 11)\n",
            "Dropped: 6392 (16, 11)\n",
            "Dropped: 6394 (16, 9)\n",
            "Dropped: 6402 (16, 10)\n",
            "Dropped: 6404 (16, 14)\n",
            "Dropped: 6412 (16, 15)\n",
            "Dropped: 6421 (16, 13)\n",
            "Dropped: 6430 (16, 11)\n",
            "Dropped: 6449 (16, 13)\n",
            "Dropped: 6487 (16, 12)\n",
            "Dropped: 6505 (16, 15)\n",
            "Dropped: 6509 (16, 8)\n",
            "Dropped: 6511 (16, 15)\n",
            "Dropped: 6517 (16, 15)\n",
            "Dropped: 6553 (16, 13)\n",
            "Dropped: 6571 (16, 12)\n",
            "Dropped: 6576 (16, 12)\n",
            "Dropped: 6585 (16, 7)\n",
            "Dropped: 6587 (16, 7)\n",
            "Dropped: 6588 (16, 12)\n",
            "Dropped: 6593 (16, 7)\n",
            "Dropped: 6607 (16, 11)\n",
            "Dropped: 6611 (16, 12)\n",
            "Dropped: 6660 (16, 15)\n",
            "Dropped: 6662 (16, 8)\n",
            "Dropped: 6713 (16, 13)\n",
            "Dropped: 6718 (16, 15)\n",
            "Dropped: 6739 (16, 11)\n",
            "Dropped: 6741 (16, 15)\n",
            "Dropped: 6753 (16, 15)\n",
            "Dropped: 6803 (16, 13)\n",
            "Dropped: 6810 (16, 12)\n",
            "Dropped: 6811 (16, 14)\n",
            "Dropped: 6843 (16, 7)\n",
            "Dropped: 6859 (16, 11)\n",
            "Dropped: 6879 (16, 11)\n",
            "Dropped: 6886 (16, 13)\n",
            "Dropped: 6897 (16, 14)\n",
            "Dropped: 6907 (16, 13)\n",
            "Dropped: 6952 (16, 11)\n",
            "Dropped: 6975 (16, 11)\n",
            "Dropped: 6976 (16, 12)\n",
            "Dropped: 7016 (16, 13)\n",
            "Dropped: 7021 (16, 11)\n",
            "Dropped: 7050 (16, 11)\n",
            "Dropped: 7058 (16, 8)\n",
            "Dropped: 7074 (16, 13)\n",
            "Dropped: 7076 (16, 15)\n",
            "Dropped: 7079 (16, 10)\n",
            "Dropped: 7084 (16, 11)\n",
            "Dropped: 7094 (16, 13)\n",
            "Dropped: 7102 (16, 12)\n",
            "Dropped: 7106 (16, 15)\n",
            "Dropped: 7116 (16, 12)\n",
            "Dropped: 7121 (16, 15)\n",
            "Dropped: 7124 (16, 8)\n",
            "Dropped: 7138 (16, 12)\n",
            "Dropped: 7141 (16, 13)\n",
            "Dropped: 7156 (16, 15)\n",
            "Dropped: 7157 (16, 12)\n",
            "Dropped: 7158 (16, 13)\n",
            "Dropped: 7200 (16, 13)\n",
            "Dropped: 7219 (16, 7)\n",
            "Dropped: 7220 (16, 8)\n",
            "Dropped: 7221 (16, 13)\n",
            "Dropped: 7231 (16, 13)\n",
            "Dropped: 7249 (16, 13)\n",
            "Dropped: 7271 (16, 11)\n",
            "Dropped: 7296 (16, 13)\n",
            "Dropped: 7302 (16, 11)\n",
            "Dropped: 7331 (16, 8)\n",
            "Dropped: 7345 (16, 11)\n",
            "Dropped: 7355 (16, 9)\n",
            "Dropped: 7370 (16, 11)\n",
            "Dropped: 7372 (16, 14)\n",
            "Dropped: 7373 (16, 13)\n",
            "Dropped: 7401 (16, 13)\n",
            "Dropped: 7406 (16, 13)\n",
            "Dropped: 7421 (16, 9)\n",
            "Dropped: 7430 (16, 9)\n",
            "Dropped: 7440 (16, 12)\n",
            "Dropped: 7442 (16, 7)\n",
            "Dropped: 7465 (16, 15)\n",
            "Dropped: 7504 (16, 14)\n",
            "Dropped: 7539 (16, 12)\n",
            "Dropped: 7544 (16, 10)\n",
            "Dropped: 7553 (16, 14)\n",
            "Dropped: 7560 (16, 9)\n",
            "Dropped: 7579 (16, 11)\n",
            "Dropped: 7589 (16, 11)\n",
            "Dropped: 7602 (16, 13)\n",
            "Dropped: 7614 (16, 13)\n",
            "Dropped: 7619 (16, 10)\n",
            "Dropped: 7632 (16, 15)\n",
            "Dropped: 7648 (16, 13)\n",
            "Dropped: 7657 (16, 14)\n",
            "Dropped: 7667 (16, 7)\n",
            "Dropped: 7671 (16, 13)\n",
            "Dropped: 7679 (16, 10)\n",
            "Dropped: 7687 (16, 12)\n",
            "Dropped: 7688 (16, 15)\n",
            "Dropped: 7693 (16, 12)\n",
            "Dropped: 7705 (16, 13)\n",
            "Dropped: 7715 (16, 13)\n",
            "Dropped: 7723 (16, 11)\n",
            "Dropped: 7734 (16, 14)\n",
            "Dropped: 7737 (16, 14)\n",
            "Dropped: 7743 (16, 13)\n",
            "Dropped: 7747 (16, 11)\n",
            "Dropped: 7757 (16, 7)\n",
            "Dropped: 7764 (16, 11)\n",
            "Dropped: 7775 (16, 11)\n",
            "Dropped: 7780 (16, 15)\n",
            "Dropped: 7799 (16, 15)\n",
            "Dropped: 7805 (16, 14)\n",
            "Dropped: 7811 (16, 13)\n",
            "Dropped: 7900 (16, 14)\n",
            "Dropped: 7918 (16, 14)\n",
            "Dropped: 7947 (16, 15)\n",
            "Dropped: 7966 (16, 13)\n",
            "Dropped: 7975 (16, 3)\n",
            "Dropped: 7984 (16, 12)\n",
            "Dropped: 7994 (16, 10)\n",
            "Dropped: 7998 (16, 14)\n",
            "Dropped: 8020 (16, 10)\n",
            "Dropped: 8069 (16, 10)\n",
            "Dropped: 8084 (16, 13)\n",
            "Dropped: 8090 (16, 13)\n",
            "Dropped: 8120 (16, 6)\n",
            "Dropped: 8150 (16, 14)\n",
            "Dropped: 8164 (16, 12)\n",
            "Dropped: 8168 (16, 13)\n",
            "Dropped: 8169 (16, 13)\n",
            "Dropped: 8175 (16, 12)\n",
            "Dropped: 8177 (16, 13)\n",
            "Dropped: 8190 (16, 11)\n",
            "Dropped: 8192 (16, 11)\n",
            "Dropped: 8208 (16, 15)\n",
            "Dropped: 8222 (16, 11)\n",
            "Dropped: 8228 (16, 11)\n",
            "Dropped: 8236 (16, 14)\n",
            "Dropped: 8247 (16, 14)\n",
            "Dropped: 8249 (16, 13)\n",
            "Dropped: 8251 (16, 11)\n",
            "Dropped: 8310 (16, 9)\n",
            "Dropped: 8337 (16, 15)\n",
            "Dropped: 8341 (16, 15)\n",
            "Dropped: 8349 (16, 13)\n",
            "Dropped: 8352 (16, 15)\n",
            "Dropped: 8362 (16, 12)\n",
            "Dropped: 8363 (16, 13)\n",
            "Dropped: 8365 (16, 14)\n",
            "Dropped: 8387 (16, 7)\n",
            "Dropped: 8400 (16, 15)\n",
            "Dropped: 8444 (16, 12)\n",
            "Dropped: 8446 (16, 14)\n",
            "Dropped: 8459 (16, 14)\n",
            "Dropped: 8487 (16, 12)\n",
            "Dropped: 8502 (16, 12)\n",
            "Dropped: 8504 (16, 11)\n",
            "Dropped: 8521 (16, 13)\n",
            "Dropped: 8546 (16, 13)\n",
            "Dropped: 8557 (16, 13)\n",
            "Dropped: 8568 (16, 13)\n",
            "Dropped: 8575 (16, 14)\n",
            "Dropped: 8576 (16, 13)\n",
            "Dropped: 8580 (16, 15)\n",
            "Dropped: 8596 (16, 7)\n",
            "Dropped: 8605 (16, 9)\n",
            "Dropped: 8621 (16, 12)\n",
            "Dropped: 8623 (16, 14)\n",
            "Dropped: 8626 (16, 11)\n",
            "Dropped: 8629 (16, 15)\n",
            "Dropped: 8660 (16, 12)\n",
            "Dropped: 8662 (16, 15)\n",
            "Dropped: 8664 (16, 13)\n",
            "Dropped: 8678 (16, 13)\n",
            "Dropped: 8689 (16, 15)\n",
            "Dropped: 8717 (16, 12)\n",
            "Dropped: 8730 (16, 15)\n",
            "Dropped: 8736 (16, 8)\n",
            "Dropped: 8737 (16, 14)\n",
            "Dropped: 8769 (16, 10)\n",
            "Dropped: 8776 (16, 13)\n",
            "Dropped: 8780 (16, 10)\n",
            "Dropped: 8791 (16, 12)\n",
            "Dropped: 8839 (16, 7)\n",
            "Dropped: 8850 (16, 13)\n",
            "Dropped: 8858 (16, 11)\n",
            "Dropped: 8873 (16, 8)\n",
            "Dropped: 8894 (16, 8)\n",
            "Dropped: 8895 (16, 8)\n",
            "Dropped: 8911 (16, 13)\n",
            "Dropped: 8913 (16, 12)\n",
            "Dropped: 8921 (16, 14)\n",
            "Dropped: 8955 (16, 8)\n",
            "Dropped: 8956 (16, 15)\n",
            "Dropped: 8969 (16, 8)\n",
            "Dropped: 8976 (16, 9)\n",
            "Dropped: 8990 (16, 15)\n",
            "Dropped: 8997 (16, 15)\n",
            "Dropped: 9023 (16, 13)\n",
            "Dropped: 9037 (16, 14)\n",
            "Dropped: 9038 (16, 14)\n",
            "Dropped: 9052 (16, 14)\n",
            "Dropped: 9063 (16, 14)\n",
            "Dropped: 9071 (16, 10)\n",
            "Dropped: 9080 (16, 11)\n",
            "Dropped: 9098 (16, 15)\n",
            "Dropped: 9109 (16, 15)\n",
            "Dropped: 9132 (16, 13)\n",
            "Dropped: 9152 (16, 13)\n",
            "Dropped: 9162 (16, 15)\n",
            "Dropped: 9164 (16, 12)\n",
            "Dropped: 9175 (16, 12)\n",
            "Dropped: 9189 (16, 15)\n",
            "Dropped: 9208 (16, 12)\n",
            "Dropped: 9221 (16, 15)\n",
            "Dropped: 9224 (16, 13)\n",
            "Dropped: 9228 (16, 9)\n",
            "Dropped: 9241 (16, 11)\n",
            "Dropped: 9279 (16, 7)\n",
            "Dropped: 9282 (16, 15)\n",
            "Dropped: 9283 (16, 13)\n",
            "Dropped: 9301 (16, 7)\n",
            "Dropped: 9313 (16, 14)\n",
            "Dropped: 9320 (16, 9)\n",
            "Dropped: 9339 (16, 13)\n",
            "Dropped: 9351 (16, 13)\n",
            "Dropped: 9392 (16, 5)\n",
            "Dropped: 9422 (16, 9)\n",
            "Dropped: 9440 (16, 7)\n",
            "Dropped: 9445 (16, 10)\n",
            "Dropped: 9459 (16, 14)\n",
            "Dropped: 9465 (16, 11)\n",
            "Dropped: 9476 (16, 15)\n",
            "Dropped: 9497 (16, 5)\n",
            "Dropped: 9498 (16, 10)\n",
            "Dropped: 9499 (16, 12)\n",
            "Dropped: 9535 (16, 14)\n",
            "Dropped: 9544 (16, 12)\n",
            "Dropped: 9549 (16, 14)\n",
            "Dropped: 9573 (16, 11)\n",
            "Dropped: 9581 (16, 9)\n",
            "Dropped: 9588 (16, 12)\n",
            "Dropped: 9592 (16, 12)\n",
            "Dropped: 9599 (16, 9)\n",
            "Dropped: 9604 (16, 14)\n",
            "Dropped: 9613 (16, 9)\n",
            "Dropped: 9615 (16, 9)\n",
            "Dropped: 9621 (16, 12)\n",
            "Dropped: 9630 (16, 13)\n",
            "Dropped: 9637 (16, 3)\n",
            "Dropped: 9641 (16, 13)\n",
            "Dropped: 9714 (16, 11)\n",
            "Dropped: 9720 (16, 8)\n",
            "Dropped: 9722 (16, 6)\n",
            "Dropped: 9733 (16, 10)\n",
            "Dropped: 9734 (16, 15)\n",
            "Dropped: 9735 (16, 13)\n",
            "Dropped: 9761 (16, 11)\n",
            "Dropped: 9771 (16, 15)\n",
            "Dropped: 9780 (16, 13)\n",
            "Dropped: 9781 (16, 10)\n",
            "Dropped: 9811 (16, 13)\n",
            "Dropped: 9827 (16, 15)\n",
            "Dropped: 9833 (16, 12)\n",
            "Dropped: 9860 (16, 15)\n",
            "Dropped: 9864 (16, 12)\n",
            "Dropped: 9876 (16, 8)\n",
            "Dropped: 9881 (16, 14)\n",
            "Dropped: 9886 (16, 13)\n",
            "Dropped: 9911 (16, 10)\n",
            "Dropped: 9912 (16, 7)\n",
            "Dropped: 9922 (16, 14)\n",
            "Dropped: 9927 (16, 13)\n",
            "Dropped: 9952 (16, 6)\n",
            "Dropped: 9954 (16, 14)\n",
            "Dropped: 9958 (16, 15)\n",
            "Dropped: 9985 (16, 8)\n",
            "Dropped: 10003 (16, 12)\n",
            "Dropped: 10018 (16, 7)\n",
            "Dropped: 10045 (16, 7)\n",
            "Dropped: 10061 (16, 10)\n",
            "Dropped: 10063 (16, 13)\n",
            "Dropped: 10077 (16, 15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save features and truth vector (y) sets to disk\n",
        "np.savez(feature_sets_file, \n",
        "         x_train=x_train, \n",
        "         y_train=y_train, \n",
        "         x_val=x_val, \n",
        "         y_val=y_val, \n",
        "         x_test=x_test, \n",
        "         y_test=y_test)"
      ],
      "metadata": {
        "id": "W7-5HTD1ixEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST: Load features\n",
        "feature_sets = np.load(feature_sets_file)\n",
        "feature_sets.files"
      ],
      "metadata": {
        "id": "XWQmyjUdkJ9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(feature_sets['x_train'])"
      ],
      "metadata": {
        "id": "-i8ExtvTkNsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(feature_sets['y_val'])"
      ],
      "metadata": {
        "id": "vG9rNkV6kQzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers, models"
      ],
      "metadata": {
        "id": "cozzrncn2qeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create list of all targets (minus background noise)\n",
        "# dataset_path = 'C:\\\\PATH\\\\TO\\\\speech_commands_dataset'\n",
        "# all_targets = all_targets = [name for name in listdir(dataset_path) if isdir(join(dataset_path, name))]\n",
        "# all_targets.remove('_background_noise_')\n",
        "print(all_targets)"
      ],
      "metadata": {
        "id": "RuUGYm9P2wEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Settings\n",
        "feature_sets_path = './'\n",
        "feature_sets_filename = 'all_targets_mfcc_sets.npz'\n",
        "model_filename = 'wake_word_stop_model.h5'\n",
        "wake_word = 'stop'"
      ],
      "metadata": {
        "id": "yUG41pz228KQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load feature sets\n",
        "feature_sets = np.load(join(feature_sets_path, feature_sets_filename))\n",
        "print(feature_sets.files)"
      ],
      "metadata": {
        "id": "Mymb9BkF3N1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assign feature sets\n",
        "x_train = feature_sets['x_train']\n",
        "y_train = feature_sets['y_train']\n",
        "x_val = feature_sets['x_val']\n",
        "y_val = feature_sets['y_val']\n",
        "x_test = feature_sets['x_test']\n",
        "y_test = feature_sets['y_test']"
      ],
      "metadata": {
        "id": "CaVkUEpw3XLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at tensor dimensions\n",
        "print(x_train.shape)\n",
        "print(x_val.shape)\n",
        "print(x_test.shape)"
      ],
      "metadata": {
        "id": "_qr4fKAI3aXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Peek at labels\n",
        "print(y_val)"
      ],
      "metadata": {
        "id": "Zyvd-FwT3gNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert ground truth arrays to one wake word (1) and 'other' (0)\n",
        "wake_word_index = all_targets.index(wake_word)\n",
        "y_train = np.equal(y_train, wake_word_index).astype('float64')\n",
        "y_val = np.equal(y_val, wake_word_index).astype('float64')\n",
        "y_test = np.equal(y_test, wake_word_index).astype('float64')"
      ],
      "metadata": {
        "id": "YmTxJrzP3kMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Peek at labels after conversion\n",
        "print(y_val)"
      ],
      "metadata": {
        "id": "F1jwif_F3oKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What percentage of 'stop' appear in validation labels\n",
        "print(sum(y_val) / len(y_val))\n",
        "print(1 - sum(y_val) / len(y_val))"
      ],
      "metadata": {
        "id": "VDeJQKnX3wzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View the dimensions of our input data\n",
        "print(x_train.shape)"
      ],
      "metadata": {
        "id": "WVem2mGf32eU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN for TF expects (batch, height, width, channels)\n",
        "# So we reshape the input tensors with a \"color\" channel of 1\n",
        "x_train = x_train.reshape(x_train.shape[0], \n",
        "                          x_train.shape[1], \n",
        "                          x_train.shape[2], \n",
        "                          1)\n",
        "x_val = x_val.reshape(x_val.shape[0], \n",
        "                      x_val.shape[1], \n",
        "                      x_val.shape[2], \n",
        "                      1)\n",
        "x_test = x_test.reshape(x_test.shape[0], \n",
        "                        x_test.shape[1], \n",
        "                        x_test.shape[2], \n",
        "                        1)\n",
        "print(x_train.shape)\n",
        "print(x_val.shape)\n",
        "print(x_test.shape)"
      ],
      "metadata": {
        "id": "9UkD6FoK34RS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Input shape for CNN is size of MFCC of 1 sample\n",
        "sample_shape = x_test.shape[1:]\n",
        "print(sample_shape)"
      ],
      "metadata": {
        "id": "ArsxMONZ5K6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build model\n",
        "# Based on: https://www.geeksforgeeks.org/python-image-classification-using-keras/\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, \n",
        "                        (2, 2), \n",
        "                        activation='relu',\n",
        "                        input_shape=sample_shape))\n",
        "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(layers.Conv2D(32, (2, 2), activation='relu'))\n",
        "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(layers.Conv2D(64, (2, 2), activation='relu'))\n",
        "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Classifier\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "HL1Dzaz65Oky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display model\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "KJCiCQFn5TgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add training parameters to model\n",
        "model.compile(loss='binary_crossentropy', \n",
        "              optimizer='rmsprop', \n",
        "              metrics=['acc'])"
      ],
      "metadata": {
        "id": "uauWW3jY5mxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train\n",
        "history = model.fit(x_train, \n",
        "                    y_train, \n",
        "                    epochs=30, \n",
        "                    batch_size=100, \n",
        "                    validation_data=(x_val, y_val))"
      ],
      "metadata": {
        "id": "q-cI2Sj-5sDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot results\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DEZYreXK9r5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model as a file\n",
        "models.save_model(model, model_filename)"
      ],
      "metadata": {
        "id": "hp1knp7b-nn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# See which are 'stop'\n",
        "for idx, y in enumerate(y_test):\n",
        "    if y == 1:\n",
        "        print(idx)"
      ],
      "metadata": {
        "id": "x4SrQhL6-pWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TEST: Load model and run it against test set\n",
        "model = models.load_model(model_filename)\n",
        "for i in range(100, 110):\n",
        "    print('Answer:', y_test[i], ' Prediction:', model.predict(np.expand_dims(x_test[i], 0)))"
      ],
      "metadata": {
        "id": "7zYkWB5Q-tA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model with test set\n",
        "model.evaluate(x=x_test, y=y_test)"
      ],
      "metadata": {
        "id": "E6F_FK20_ZlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import lite"
      ],
      "metadata": {
        "id": "2l0d9jjPBvy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "keras_model_filename = 'wake_word_stop_model.h5'\n",
        "tflite_filename = 'wake_word_stop_lite.tflite'"
      ],
      "metadata": {
        "id": "YdpSMMkiB7XO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert model to TF Lite model\n",
        "model = models.load_model(keras_model_filename)\n",
        "converter = lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "open(tflite_filename, 'wb').write(tflite_model)"
      ],
      "metadata": {
        "id": "9NQGw5YVB-Nh"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNKClyrnvQeqQ76m1tjDaQy",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}